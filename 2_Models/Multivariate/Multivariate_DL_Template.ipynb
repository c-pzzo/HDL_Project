{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae1ca6a",
   "metadata": {},
   "source": [
    "# About\n",
    "General template for univariate data analysis with DL Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5216b5b",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6499a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"/home/cesar/Python_NBs/HDL_Project/HDL_Project/global_fv.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7492c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Layers\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "\n",
    "# Optimizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Checkpoint\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4c94f",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_tuning = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4aae2d",
   "metadata": {},
   "source": [
    "# User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for Bidirectional-LSTM\n",
    "\n",
    "def design_model(n_steps, n_features, lr):\n",
    "    \"\"\"\n",
    "    n_steps: Number of steps\n",
    "    n_features: Number of features\n",
    "    lr: Learning rate\n",
    "    \"\"\"\n",
    "    model = Sequential(name = \"Bidirectional-LSTM-model\")\n",
    "    \n",
    "    # Number of neurons (nodes) are just about greater than the number of features.\n",
    "    # Rule of thumb is for number of neurons to be about 2/3 of the input    \n",
    "    num_neurons = int(np.ceil(n_features *2 /3))\n",
    "    \n",
    "    # It'd be best for the number of neurons to be in the scale of 2^n for computational purposes.\n",
    "    if(False):\n",
    "        while num_neurons > 2**i:\n",
    "            i += 1\n",
    "\n",
    "        num_neurons = 2**i   \n",
    "    \n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(num_neurons, activation= 'relu' ), input_shape=(n_steps, n_features)))\n",
    "    \n",
    "    # Output layer with one neuron to a model instance (to return one output)\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "    # Regularization prevents the learning process to completely fit the model to the training data which can lead to overfitting.\n",
    "    # The most common regularization method is dropout.\n",
    "    #model.add(layers.Dropout(0.1))    \n",
    "    \n",
    "    # Optimizer\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,  metrics=['mae'], loss= 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729bfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_tuning_lr(X, y, learning_rate, num_epochs, bs, model):\n",
    "    y_axis_name = \"Learning rate\"\n",
    "    \n",
    "    #train the model on the training data\n",
    "    history = model.fit(X, y, epochs=num_epochs, batch_size=bs, verbose = 0, validation_split = 0.2)\n",
    "\n",
    "    # plot learning curves\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title(\"Learning Rate: {}\".format(str(learning_rate)) )\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=\"upper right\")    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb11b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_tuning_batches(X, y, learning_rate, num_epochs, bs, model):\n",
    "    \n",
    "    #train the model on the training data\n",
    "    history = model.fit(X, y, epochs=num_epochs, batch_size = bs, verbose=0, validation_split = 0.2)\n",
    "    \n",
    "    # plot learning curves\n",
    "    plt.plot(history.history['mae'], label='train')\n",
    "    plt.plot(history.history['val_mae'], label='validation')\n",
    "    plt.title('Batches = ' + str(bs))\n",
    "    plt.ylabel('mae')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ee121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_es_checkpoint(X, y, learning_rate, num_epochs, bs, model):\n",
    "    checkpoint_name = 'U_Weights_-{epoch:03d}--{val_loss:.5f}.hdf5'\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=30)\n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_name, monitor='mae', verbose=0, save_best_only=True, mode='min')\n",
    "    history = NN_model.fit(X, y, epochs=num_epochs, batch_size=bs, validation_split = 0.20, verbose = 0, callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1496",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a420121",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_number = 'SE'\n",
    "target_name = 'pm25'\n",
    "# Number of time steps per sample\n",
    "n_steps = 24\n",
    "\n",
    "# Meteorological parameters\n",
    "col_names = [i[0] for i in qdata(\"select meteorological_code from cat_meteorological_params\")]\n",
    "\n",
    "# Default neccesary columns\n",
    "cols = \"datetime, \" + target_name\n",
    "\n",
    "# Columns\n",
    "for i in col_names:\n",
    "    cols = cols + \", \" + str(i)\n",
    "\n",
    "print(cols)\n",
    "    \n",
    "# Where filter:\n",
    "where_txt = \"where datetime >= \\'2021-01-01\\'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41fa7e",
   "metadata": {},
   "source": [
    "## Creating samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf31296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing class\n",
    "main_processed_df = univariate_samples(station_number, cols, where_txt)\n",
    "\n",
    "# Execution of processing functions\n",
    "#initial_df = main_processed_df.initial_df()\n",
    "\n",
    "# Samples numpy.ndarray object \n",
    "X, y = main_processed_df.samples_creation(n_steps, target_name)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "n_features = np.shape(X)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the three-dimensional structure of the input samples\n",
    "print(np.shape(X))\n",
    "print()\n",
    "print(\"* The 1st dimension is the number of samples, in this case: {}\".format(np.shape(X)[0])) \n",
    "print(\"* The 2nd dimension is the number of time steps per sample, in this case {}, the value specified to the function.\".format(np.shape(X)[1]))\n",
    "print(\"* The 3rd dimension specifies the number of parallel time series —or the number of variables— in this case {}.\".format(np.shape(X)[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc387ed9",
   "metadata": {},
   "source": [
    "# Data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "NN_model = design_model(n_steps, n_features, learning_rate)\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "#fixed number of batches\n",
    "batch_size = 8\n",
    "\n",
    "if manual_tuning == True:\n",
    "    # List of learning rates for  testing\n",
    "    learning_rates = [1, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5]\n",
    "\n",
    "    plt.figure(figsize=(14,12))\n",
    "    plt.ylim([150, 200])\n",
    "    plt.subplots_adjust(bottom=0.1, top=1.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i in range(len(learning_rates)): \n",
    "        plt.subplot(len(learning_rates), 1, (i+1))\n",
    "        manual_tuning_lr(X, y, learning_rates[i], num_epochs, batch_size, design_model(n_steps, n_features, learning_rate))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdad7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "#fixed learning rate value\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "if manual_tuning == True:\n",
    "    #fixed number of batches\n",
    "    batch_size = [8, 16, 32, 64, 128] \n",
    "    \n",
    "    plt.figure(figsize=(14,12))\n",
    "    plt.subplots_adjust(bottom=0.1, top=1.4)\n",
    "    \n",
    "    for i in range(len(batch_size)): \n",
    "        plt.subplot(len(batch_size), 1, (i+1))\n",
    "        manual_tuning_batches(X, y, learning_rate, num_epochs, batch_size[i], design_model(n_steps, n_features, learning_rate))\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b138b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with early stopping\n",
    "\n",
    "# Maximum number of epochs\n",
    "num_epochs = 300\n",
    "\n",
    "#fixed learning rate value\n",
    "learning_rate = 0.01\n",
    "\n",
    "#fixed number of batches\n",
    "batch_size = 4\n",
    "\n",
    "history = fit_model_es_checkpoint(X, y, learning_rate, num_epochs, batch_size, design_model(n_steps, n_features, learning_rate))\n",
    "          \n",
    "#plotting\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.subplots_adjust(bottom=0.1, top=1.4)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plot learning curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "# plot learning curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='train')\n",
    "plt.plot(history.history['val_mae'], label='validation')\n",
    "plt.title(\"MAE\")\n",
    "plt.ylabel('mae')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.cla()\n",
    "plt.clf()\n",
    "plt.close('all')\n",
    "\n",
    "print(\"Min training Loss:\", min(history.history[\"loss\"]))\n",
    "print(\"Min validation Loss: \", min(history.history[\"val_loss\"]))\n",
    "print(\"\")\n",
    "print(\"Final training MAE:\", history.history['mae'][-1])\n",
    "print(\"Final validation MAE:\", history.history['val_mae'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e4c9b",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction\n",
    "x_input = np.array([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed78227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
